{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path, PurePath\n",
    "import glob\n",
    "import itertools\n",
    "\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in the data\n",
    "#specifying data directory, current folder of this file joined with the folder containing the data\n",
    "DATA_FOLDER = os.path.join(Path.cwd(), \"cifar-10-batches-py/\")\n",
    "\n",
    "#gets filenames\n",
    "batch_names = sorted(glob.glob(f\"{DATA_FOLDER}/data*\"))\n",
    "validation_path = os.path.join(DATA_FOLDER, \"test_batch\")\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, \"rb\") as f:\n",
    "        dict = pickle.load(f, encoding=\"bytes\")\n",
    "    return dict\n",
    "\n",
    "#unpickles and divides the cifar batches into data and labels as a list of dictionaries,\n",
    "#also reshapes the 1000*3072 (n_pictures*color_channels) arrays into 3*32*32(n_channels, width, height)\n",
    "def read_data_label_set(filepaths):\n",
    "    \n",
    "    if isinstance(filepaths, list):\n",
    "    \n",
    "        li=[]\n",
    "        for file in filepaths:\n",
    "            unpacked = unpickle(file)\n",
    "            data = torch.FloatTensor(list(map(lambda x : np.reshape(x, (3, 32, 32)), unpacked[b'data'])))\n",
    "            labels = torch.tensor(unpacked[b'labels'])\n",
    "            li.append({\"data\":data, \"labels\":labels})\n",
    "        return li\n",
    "    else:\n",
    "        unpacked = unpickle(filepaths)\n",
    "        data = torch.FloatTensor(list(map(lambda x : np.reshape(x, (3, 32, 32)), unpacked[b'data'])))\n",
    "        labels = torch.tensor(unpacked[b'labels'])\n",
    "        dic={\"data\":data, \"labels\":labels}\n",
    "        return dic\n",
    "\n",
    "#loads training and validtion sets\n",
    "batches = read_data_label_set(batch_names)\n",
    "validation = read_data_label_set(validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 59.,  43.,  50.,  ..., 158., 152., 148.],\n",
      "          [ 16.,   0.,  18.,  ..., 123., 119., 122.],\n",
      "          [ 25.,  16.,  49.,  ..., 118., 120., 109.],\n",
      "          ...,\n",
      "          [208., 201., 198.,  ..., 160.,  56.,  53.],\n",
      "          [180., 173., 186.,  ..., 184.,  97.,  83.],\n",
      "          [177., 168., 179.,  ..., 216., 151., 123.]],\n",
      "\n",
      "         [[ 62.,  46.,  48.,  ..., 132., 125., 124.],\n",
      "          [ 20.,   0.,   8.,  ...,  88.,  83.,  87.],\n",
      "          [ 24.,   7.,  27.,  ...,  84.,  84.,  73.],\n",
      "          ...,\n",
      "          [170., 153., 161.,  ..., 133.,  31.,  34.],\n",
      "          [139., 123., 144.,  ..., 148.,  62.,  53.],\n",
      "          [144., 129., 142.,  ..., 184., 118.,  92.]],\n",
      "\n",
      "         [[ 63.,  45.,  43.,  ..., 108., 102., 103.],\n",
      "          [ 20.,   0.,   0.,  ...,  55.,  50.,  57.],\n",
      "          [ 21.,   0.,   8.,  ...,  50.,  50.,  42.],\n",
      "          ...,\n",
      "          [ 96.,  34.,  26.,  ...,  70.,   7.,  20.],\n",
      "          [ 96.,  42.,  30.,  ...,  94.,  34.,  34.],\n",
      "          [116.,  94.,  87.,  ..., 140.,  84.,  72.]]],\n",
      "\n",
      "\n",
      "        [[[154., 126., 105.,  ...,  91.,  87.,  79.],\n",
      "          [140., 145., 125.,  ...,  96.,  77.,  71.],\n",
      "          [140., 139., 115.,  ...,  79.,  68.,  67.],\n",
      "          ...,\n",
      "          [175., 156., 154.,  ...,  42.,  61.,  93.],\n",
      "          [165., 156., 159.,  ..., 103., 123., 131.],\n",
      "          [163., 158., 163.,  ..., 143., 143., 143.]],\n",
      "\n",
      "         [[177., 137., 104.,  ...,  95.,  90.,  81.],\n",
      "          [160., 153., 125.,  ...,  99.,  80.,  73.],\n",
      "          [155., 146., 115.,  ...,  82.,  70.,  69.],\n",
      "          ...,\n",
      "          [167., 154., 160.,  ...,  34.,  53.,  83.],\n",
      "          [154., 152., 161.,  ...,  93., 114., 121.],\n",
      "          [148., 148., 156.,  ..., 133., 134., 133.]],\n",
      "\n",
      "         [[187., 136.,  95.,  ...,  71.,  71.,  70.],\n",
      "          [169., 154., 118.,  ...,  78.,  62.,  61.],\n",
      "          [164., 149., 112.,  ...,  64.,  55.,  55.],\n",
      "          ...,\n",
      "          [166., 160., 170.,  ...,  36.,  57.,  91.],\n",
      "          [128., 130., 142.,  ...,  96., 120., 131.],\n",
      "          [120., 122., 133.,  ..., 139., 142., 144.]]],\n",
      "\n",
      "\n",
      "        [[[255., 253., 253.,  ..., 253., 253., 253.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 254., 254.,  ..., 254., 254., 254.],\n",
      "          ...,\n",
      "          [113., 111., 105.,  ...,  72.,  72.,  72.],\n",
      "          [111., 104.,  99.,  ...,  68.,  70.,  78.],\n",
      "          [106.,  99.,  95.,  ...,  78.,  79.,  80.]],\n",
      "\n",
      "         [[255., 253., 253.,  ..., 253., 253., 253.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 254., 254.,  ..., 254., 254., 254.],\n",
      "          ...,\n",
      "          [120., 118., 112.,  ...,  81.,  80.,  80.],\n",
      "          [118., 111., 106.,  ...,  75.,  76.,  84.],\n",
      "          [113., 106., 102.,  ...,  85.,  85.,  86.]],\n",
      "\n",
      "         [[255., 253., 253.,  ..., 253., 253., 253.],\n",
      "          [255., 255., 255.,  ..., 255., 255., 255.],\n",
      "          [255., 254., 254.,  ..., 254., 254., 254.],\n",
      "          ...,\n",
      "          [112., 111., 106.,  ...,  80.,  79.,  79.],\n",
      "          [110., 104.,  98.,  ...,  73.,  75.,  82.],\n",
      "          [105.,  98.,  94.,  ...,  83.,  83.,  84.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 71.,  60.,  74.,  ..., 251., 255., 191.],\n",
      "          [ 89.,  80.,  67.,  ..., 250., 255., 181.],\n",
      "          [ 74.,  69.,  68.,  ..., 246., 254., 193.],\n",
      "          ...,\n",
      "          [ 84.,  80.,  80.,  ...,  41.,  55.,  61.],\n",
      "          [ 67.,  65.,  62.,  ...,  63.,  68.,  71.],\n",
      "          [ 66.,  66.,  65.,  ...,  80.,  83.,  84.]],\n",
      "\n",
      "         [[ 77.,  69.,  83.,  ..., 251., 255., 192.],\n",
      "          [ 96.,  89.,  77.,  ..., 248., 254., 181.],\n",
      "          [ 80.,  77.,  77.,  ..., 245., 254., 195.],\n",
      "          ...,\n",
      "          [ 85.,  81.,  80.,  ...,  43.,  55.,  62.],\n",
      "          [ 66.,  64.,  61.,  ...,  63.,  66.,  70.],\n",
      "          [ 43.,  44.,  42.,  ...,  73.,  75.,  77.]],\n",
      "\n",
      "         [[ 44.,  35.,  56.,  ..., 253., 255., 186.],\n",
      "          [ 62.,  58.,  51.,  ..., 249., 254., 170.],\n",
      "          [ 46.,  50.,  53.,  ..., 248., 254., 183.],\n",
      "          ...,\n",
      "          [ 84.,  81.,  80.,  ...,  48.,  59.,  63.],\n",
      "          [ 67.,  64.,  61.,  ...,  65.,  67.,  67.],\n",
      "          [ 28.,  28.,  27.,  ...,  68.,  69.,  68.]]],\n",
      "\n",
      "\n",
      "        [[[250., 254., 211.,  ..., 188., 255., 255.],\n",
      "          [250., 255., 213.,  ..., 189., 255., 255.],\n",
      "          [250., 255., 213.,  ..., 186., 255., 255.],\n",
      "          ...,\n",
      "          [255., 254., 213.,  ..., 211., 255., 255.],\n",
      "          [255., 252., 217.,  ..., 214., 255., 255.],\n",
      "          [255., 253., 218.,  ..., 214., 255., 255.]],\n",
      "\n",
      "         [[255., 253., 224.,  ..., 195., 255., 254.],\n",
      "          [255., 254., 225.,  ..., 198., 255., 254.],\n",
      "          [255., 254., 226.,  ..., 198., 255., 254.],\n",
      "          ...,\n",
      "          [254., 254., 224.,  ..., 211., 255., 254.],\n",
      "          [254., 253., 225.,  ..., 213., 255., 254.],\n",
      "          [254., 253., 221.,  ..., 212., 255., 254.]],\n",
      "\n",
      "         [[255., 254., 220.,  ..., 193., 255., 255.],\n",
      "          [255., 255., 222.,  ..., 195., 255., 255.],\n",
      "          [255., 255., 222.,  ..., 194., 255., 255.],\n",
      "          ...,\n",
      "          [255., 255., 221.,  ..., 215., 255., 254.],\n",
      "          [255., 253., 222.,  ..., 216., 255., 254.],\n",
      "          [255., 253., 220.,  ..., 215., 255., 254.]]],\n",
      "\n",
      "\n",
      "        [[[ 62.,  61.,  60.,  ...,  64.,  82.,  62.],\n",
      "          [ 62.,  63.,  61.,  ...,  77., 114.,  64.],\n",
      "          [ 67.,  78., 115.,  ..., 100., 119.,  63.],\n",
      "          ...,\n",
      "          [161., 159., 159.,  ..., 152., 157., 156.],\n",
      "          [163., 161., 162.,  ..., 162., 161., 161.],\n",
      "          [169., 167., 167.,  ..., 167., 167., 167.]],\n",
      "\n",
      "         [[ 55.,  55.,  55.,  ...,  58.,  75.,  55.],\n",
      "          [ 56.,  52.,  48.,  ...,  69., 107.,  57.],\n",
      "          [ 59.,  62.,  93.,  ...,  92., 111.,  56.],\n",
      "          ...,\n",
      "          [192., 190., 190.,  ..., 167., 190., 192.],\n",
      "          [195., 192., 193.,  ..., 191., 193., 193.],\n",
      "          [201., 198., 198.,  ..., 198., 198., 198.]],\n",
      "\n",
      "         [[  7.,   7.,   6.,  ...,  10.,  28.,   9.],\n",
      "          [  7.,   8.,   6.,  ...,  28.,  65.,  12.],\n",
      "          [ 15.,  28.,  66.,  ...,  58.,  74.,  12.],\n",
      "          ...,\n",
      "          [125., 123., 123.,  ..., 123., 128., 123.],\n",
      "          [127., 125., 126.,  ..., 128., 127., 126.],\n",
      "          [132., 130., 131.,  ..., 130., 130., 131.]]]])\n"
     ]
    }
   ],
   "source": [
    "#preprocessing/normalization step\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a convolutional network class\n",
    "class ConvNet(nn.Module):\n",
    "    # conv reduction: (W-F+2P)/S +1\n",
    "    # W:input, F:filter, P:padding, S:stride\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "\n",
    "            # 3*32*32\n",
    "            nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3,\n",
    "                      padding=1, stride=1),  # ->128*32*32\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # ->128*16*16\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # 128*16*16\n",
    "            nn.Conv2d(in_channels=128, out_channels=512,\n",
    "                      kernel_size=3, padding=1, stride=1),  # ->512*16*16\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # ->512*8*8\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # ->512*8*8\n",
    "            nn.Conv2d(in_channels=512, out_channels=512,\n",
    "                      kernel_size=3, padding=1, stride=1),  # ->512*8*8\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # ->512*4*4\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # ->512*4*4\n",
    "            nn.Conv2d(in_channels=512, out_channels=512,\n",
    "                      kernel_size=3, padding=1, stride=1),  # ->512*4*4\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # ->512*2*2\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # ->512*2*2\n",
    "            nn.Conv2d(in_channels=512, out_channels=512,\n",
    "                      kernel_size=3, padding=1, stride=1),  # ->512*2*2\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # ->512*1*1\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Flatten\n",
    "            nn.Flatten(),  # ->512\n",
    "            nn.Linear(512, 10)  # ->10\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.network(data)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): ReLU()\n",
      "    (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): ReLU()\n",
      "    (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): ReLU()\n",
      "    (15): Flatten(start_dim=1, end_dim=-1)\n",
      "    (16): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "7678474 [3456, 128, 589824, 512, 2359296, 512, 2359296, 512, 2359296, 512, 5120, 10]\n"
     ]
    }
   ],
   "source": [
    "# initializes model\n",
    "model = ConvNet()\n",
    "print(model)\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "print(sum(numel_list), numel_list)\n",
    "\n",
    "# defines optimizer, loss function and the hyperparameters\n",
    "# hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 3\n",
    "loss_meas = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model\n",
    "check_poi = torch.load(\"./cifar_m0.pt\")\n",
    "model = ConvNet()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "model.load_state_dict(check_poi[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(check_poi[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     0] loss: 0.0000\n",
      "[1,  2000] loss: 0.0006\n",
      "[1,  4000] loss: 0.0005\n",
      "[1,  6000] loss: 0.0006\n",
      "[1,  8000] loss: 0.0005\n",
      "[1,     0] loss: 0.0005\n",
      "[1,  2000] loss: 0.0007\n",
      "[1,  4000] loss: 0.0010\n",
      "[1,  6000] loss: 0.0011\n",
      "[1,  8000] loss: 0.0007\n",
      "[1,     0] loss: 0.0007\n",
      "[1,  2000] loss: 0.0006\n",
      "[1,  4000] loss: 0.0006\n",
      "[1,  6000] loss: 0.0009\n",
      "[1,  8000] loss: 0.0007\n",
      "[1,     0] loss: 0.0006\n",
      "[1,  2000] loss: 0.0005\n",
      "[1,  4000] loss: 0.0006\n",
      "[1,  6000] loss: 0.0006\n",
      "[1,  8000] loss: 0.0005\n",
      "[1,     0] loss: 0.0006\n",
      "[1,  2000] loss: 0.0006\n",
      "[1,  4000] loss: 0.0004\n",
      "[1,  6000] loss: 0.0005\n",
      "[1,  8000] loss: 0.0005\n",
      "[2,     0] loss: 0.0000\n",
      "[2,  2000] loss: 0.0006\n",
      "[2,  4000] loss: 0.0005\n",
      "[2,  6000] loss: 0.0005\n",
      "[2,  8000] loss: 0.0005\n",
      "[2,     0] loss: 0.0004\n",
      "[2,  2000] loss: 0.0006\n",
      "[2,  4000] loss: 0.0009\n",
      "[2,  6000] loss: 0.0015\n",
      "[2,  8000] loss: 0.0008\n",
      "[2,     0] loss: 0.0007\n",
      "[2,  2000] loss: 0.0004\n",
      "[2,  4000] loss: 0.0004\n",
      "[2,  6000] loss: 0.0004\n",
      "[2,  8000] loss: 0.0005\n",
      "[2,     0] loss: 0.0006\n",
      "[2,  2000] loss: 0.0005\n",
      "[2,  4000] loss: 0.0005\n",
      "[2,  6000] loss: 0.0004\n",
      "[2,  8000] loss: 0.0005\n",
      "[2,     0] loss: 0.0007\n",
      "[2,  2000] loss: 0.0006\n",
      "[2,  4000] loss: 0.0005\n",
      "[2,  6000] loss: 0.0004\n",
      "[2,  8000] loss: 0.0006\n",
      "[3,     0] loss: 0.0000\n",
      "[3,  2000] loss: 0.0005\n",
      "[3,  4000] loss: 0.0004\n",
      "[3,  6000] loss: 0.0005\n",
      "[3,  8000] loss: 0.0005\n",
      "[3,     0] loss: 0.0004\n",
      "[3,  2000] loss: 0.0005\n",
      "[3,  4000] loss: 0.0006\n",
      "[3,  6000] loss: 0.0007\n",
      "[3,  8000] loss: 0.0007\n",
      "[3,     0] loss: 0.0005\n",
      "[3,  2000] loss: 0.0004\n",
      "[3,  4000] loss: 0.0004\n",
      "[3,  6000] loss: 0.0003\n",
      "[3,  8000] loss: 0.0005\n",
      "[3,     0] loss: 0.0004\n",
      "[3,  2000] loss: 0.0005\n",
      "[3,  4000] loss: 0.0005\n",
      "[3,  6000] loss: 0.0004\n",
      "[3,  8000] loss: 0.0005\n",
      "[3,     0] loss: 0.0003\n",
      "[3,  2000] loss: 0.0005\n",
      "[3,  4000] loss: 0.0004\n",
      "[3,  6000] loss: 0.0003\n",
      "[3,  8000] loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "#TODO improve outputs, add shuffling\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # iterates over batches\n",
    "    for batch_num, batch in enumerate(batches):\n",
    "        # forward pass\n",
    "        dat_splits = torch.split(batch[\"data\"], 100)\n",
    "        lab_splits = torch.split(batch[\"labels\"], 100)\n",
    "        \n",
    "        for part in range(len(dat_splits)):\n",
    "            #model_out = model(batch[\"data\"])\n",
    "            model_out = model(dat_splits[part])\n",
    "        \n",
    "            #loss = loss_meas(model_out, batch[\"labels\"])\n",
    "            loss = loss_meas(model_out, lab_splits[part])\n",
    "            \n",
    "            #optimization and backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss+=loss.item()\n",
    "            \n",
    "            #print statistics\n",
    "            if part*100 % 2000 == 0:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.4f' %(epoch + 1, part*100, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    #saves a model every epoch            \n",
    "    savename = \"cifar_m\"+str(epoch)+\".pt\"\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': running_loss,\n",
    "            }, savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup validation data\n",
    "dat=validation[\"data\"]\n",
    "val=validation[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of plane : 86 %\n",
      "Accuracy of   car : 96 %\n",
      "Accuracy of  bird : 57 %\n",
      "Accuracy of   cat : 72 %\n",
      "Accuracy of  deer : 61 %\n",
      "Accuracy of   dog : 46 %\n",
      "Accuracy of  frog : 81 %\n",
      "Accuracy of horse : 87 %\n",
      "Accuracy of  ship : 79 %\n",
      "Accuracy of truck : 79 %\n"
     ]
    }
   ],
   "source": [
    "#validation of the model\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(int(len(dat)/100)):\n",
    "        images, labels = dat[i*100:(i+1)*100], val[i*100:(i+1)*100]\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A cell for fun, to test taken pictures in the same folder as this file\n",
    "img = [cv.imread(file) for file in (glob.glob(os.path.join(Path.cwd(), \"*.png\"))) ]\n",
    "print(glob.glob(os.path.join(Path.cwd(), \"*.png\")))\n",
    "resized = torch.tensor([cv.resize(img[i], dsize=(32, 32), interpolation=cv.INTER_CUBIC) for i in range(len(img))])  \n",
    "transposed = torch.stack([torch.transpose(resized[i], 2, 0) for i in range(len(resized))])\n",
    "inp = transposed.float()\n",
    "\n",
    "\n",
    "\n",
    "outputs = model(inp)\n",
    "print(outputs)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
